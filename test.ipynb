{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkg5kq/.conda/envs/video2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tkg5kq/.conda/envs/video2/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "2025-02-21 06:35:51.410064: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740137751.868743  398717 cuda_dnn.cc:8593] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740137751.994596  398717 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-21 06:35:53.480455: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from modeling_florence2 import *\n",
    "from configuration_florence2 import *\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _MODELS = {\n",
    "#     \"BASE_FT\": \"microsoft/Florence-2-base-ft\"\n",
    "# }\n",
    "\n",
    "# def available_models() -> List[str]:\n",
    "#     \"\"\"Returns the names of available CLIP models\"\"\"\n",
    "#     return list(_MODELS.keys())\n",
    "    \n",
    "# def load(name: str, device: Union[str, torch.device] = \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "#     \"\"\"Load the Florence Model\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     model : torch.nn.Module\n",
    "#         The Florence Model\n",
    "#     \"\"\"\n",
    "#     # download the model section (no download needed)\n",
    "#     model_path = _MODELS[name]\n",
    "    \n",
    "#     # Build the model\n",
    "#     auto_model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, revision='refs/pr/6').to(device)\n",
    "#     processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True, revision='refs/pr/6')\n",
    "#     state_dict = auto_model.state_dict()\n",
    "\n",
    "#     config = auto_model.config\n",
    "#     model = Florence2ForConditionalGeneration(\n",
    "#         config\n",
    "#     )\n",
    "\n",
    "#     # convert_weights(model)\n",
    "#     if pretrain:\n",
    "#         model.load_state_dict(state_dict)\n",
    "\n",
    "#     if str(device) == \"cpu\":\n",
    "#         model.float()\n",
    "    \n",
    "#     return model2, model2.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models: ['BASE_FT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG projection dim 768\n"
     ]
    }
   ],
   "source": [
    "print(f'Available Models: {available_models()}')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "perceptor, vlm_state_dict, processor = load(\"BASE_FT\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modules.Visual_Prompt import visual_prompt\n",
    "import yaml\n",
    "from dotmap import DotMap\n",
    "from modules.Visual_Prompt import *\n",
    "\n",
    "class visual_prompt2(nn.Module):\n",
    "    def __init__(self, sim_head, clip_state_dict, T):\n",
    "        super().__init__()\n",
    "        self.sim_header = sim_head\n",
    "        self.T = T\n",
    "        assert sim_head in [\"meanP\", \"LSTM\", \"Transf\", \"Conv_1D\", \"Transf_cls\"]\n",
    "\n",
    "        if self.sim_header == \"LSTM\" or self.sim_header == \"Transf\" or self.sim_header == \"Transf_cls\" or self.sim_header == \"Conv_1D\":\n",
    "            embed_dim = clip_state_dict[\"text_projection\"].shape[1]\n",
    "\n",
    "            context_length = clip_state_dict[\"positional_embedding\"].shape[0]\n",
    "            # vocab_size = clip_state_dict[\"token_embedding.weight\"].shape[0]\n",
    "            transformer_width = clip_state_dict[\"ln_final.weight\"].shape[0]\n",
    "            transformer_heads = transformer_width #// 64\n",
    "\n",
    "            transformer_layers = len(\n",
    "                set(k.split(\".\")[2] for k in clip_state_dict if k.startswith(f\"transformer.resblocks\")))\n",
    "\n",
    "            self.frame_position_embeddings = nn.Embedding(context_length, embed_dim)\n",
    "        if self.sim_header == \"Transf\" :\n",
    "            self.transformer = TemporalTransformer(width=embed_dim, layers=6, heads=transformer_heads)\n",
    "            print('layer=6')\n",
    "        if self.sim_header == \"LSTM\":\n",
    "            self.lstm_visual = nn.LSTM(input_size=embed_dim, hidden_size=embed_dim,\n",
    "                                       batch_first=True, bidirectional=False, num_layers=1)\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "        if self.sim_header == \"Transf_cls\":\n",
    "            self.transformer = TAggregate(clip_length=self.T, embed_dim=embed_dim, n_layers=6)\n",
    "\n",
    "        if self.sim_header == 'Conv_1D' :\n",
    "            self.shift = nn.Conv1d(embed_dim, embed_dim, 3, padding=1, groups=embed_dim, bias=False)\n",
    "            weight = torch.zeros(embed_dim, 1, 3)\n",
    "            weight[:embed_dim // 4, 0, 0] = 1.0\n",
    "            weight[embed_dim // 4:embed_dim // 4 + embed_dim // 2, 0, 1] = 1.0\n",
    "            weight[-embed_dim // 4:, 0, 2] = 1.0\n",
    "            self.shift.weight = nn.Parameter(weight)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        elif isinstance(module, LayerNorm):\n",
    "            if 'beta' in dir(module) and 'gamma' in dir(module):\n",
    "                module.beta.data.zero_()\n",
    "                module.gamma.data.fill_(1.0)\n",
    "            else:\n",
    "                module.bias.data.zero_()\n",
    "                module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.size()\n",
    "        x = x.contiguous()\n",
    "        if self.sim_header == \"meanP\":\n",
    "            pass\n",
    "        elif self.sim_header == 'Conv_1D':\n",
    "            x_original = x\n",
    "            x = x.view(-1, c, t)\n",
    "            x = self.shift(x.float())\n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = x.type(x_original.dtype) + x_original\n",
    "\n",
    "        elif self.sim_header == \"Transf\":\n",
    "            x_original = x\n",
    "            seq_length = t\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=x.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(x.size(0), -1)\n",
    "            frame_position_embeddings = self.frame_position_embeddings(position_ids)\n",
    "            x = x + frame_position_embeddings\n",
    "\n",
    "            x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "            x = self.transformer(x)\n",
    "            x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "            x = x.type(x_original.dtype) + x_original\n",
    "\n",
    "        elif self.sim_header == \"LSTM\":\n",
    "            x_original = x\n",
    "            x, _ = self.lstm_visual(x.float())\n",
    "            self.lstm_visual.flatten_parameters()\n",
    "            x = torch.cat((x, x_original[:, x.size(1):, ...].contiguous()), dim=1)\n",
    "            x = x.type(x_original.dtype) + x_original\n",
    "        elif self.sim_header == \"Transf_cls\":\n",
    "            x_original = x\n",
    "            return self.transformer(x).type(x_original.dtype)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Unknown optimizer: {}'.format(self.sim_header))\n",
    "        return x.mean(dim=1, keepdim=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./configs/education/generic.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "config = DotMap(config)\n",
    "\n",
    "vlm_state_dict[\"text_projection\"] = torch.empty((1, perceptor.config.text_config.d_model))\n",
    "vlm_state_dict[\"positional_embedding\"] = torch.empty((perceptor.config.text_config.vocab_size,))\n",
    "vlm_state_dict[\"ln_final.weight\"] = torch.empty((perceptor.config.text_config.encoder_attention_heads,))*64\n",
    "\n",
    "# fusion_model = visual_prompt2(config.network.sim_header,vlm_state_dict,config.data.num_segments, perceptor.config.text_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(perceptor.config.vision_config.dim_embed[-1])\n",
    "print(perceptor.config.vision_config.projection_dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
